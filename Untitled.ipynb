{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "386f732f-4ccc-4539-843b-d250f3ebc17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import warnings\n",
    "from keras.src.utils.np_utils import to_categorical\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e8311c-f95a-4e25-baa7-af5903130410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDI13xA1ozWumxfaLoHDkcqn+JpmneGtY1+1ivGvZhcuD8gkxhQew7CrVt4bhhRftE7zEAHHQc/SslLq8v9Rv8ATYpxBBYW8sqsv3tqgHAPbrV166kuZ7Izp0+TRG/a3uq+G7Ro9QMlzCoJUSgiRfbPcVqXV1rk03k2unmL5QS8x4GRnivH01fV7K4lEU+8Nx+8TcSO1dhoninxPeW63El3C5WVYRA0JJbIwO+az1Wsti7pkUni22hVHvbKa3EgGGzuHTNYqQz39/f31kFeFSZN2SGKEAHjHIBxkV2et+CBqts8cN0YxncilMhWHv6VgQLqXh6U/wBoWYcpGyM6Z2SIRgkY79PxFY1FLkfLuapJvUvX32dpIY2srMyxog85M7yRjOecc1Smnhs5zdpaAyn5+XbIYHO7r1qaTXvCbzjfa69EAwVWVVAEZ6g+v4c1saaNEurnFjYalIkcWEuJt0SNk/3Tkk4755zWCcuWzT/r5nN9XlfSSP/Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('./cifar10Labels.csv', index_col=0)\n",
    "\n",
    "# View an image\n",
    "img_idx = 15\n",
    "print(labels.label[img_idx])\n",
    "Image.open('cifar10/'+str(img_idx)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ea8180-d1d3-447a-8c6a-4ad4d7568326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train, y_test = train_test_split(labels.label, test_size=0.3)\n",
    "train_idx, test_idx = y_train.index, y_test.index # Storing indexes for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6b0cbe-24d3-4d95-b058-91384bec6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading images for training \n",
    "temp = []\n",
    "for img_idx in y_train.index:\n",
    "    img_path = os.path.join('cifar10/', str(img_idx) + '.png')\n",
    "    img = np.array(Image.open(img_path)).astype('float32')        \n",
    "    temp.append(img)    \n",
    "X_train = np.stack(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326a5e26-2283-4103-ad38-494ca9a06fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading images for testing \n",
    "temp = []\n",
    "for img_idx in y_test.index:\n",
    "    img_path = os.path.join('cifar10/', str(img_idx) + '.png')\n",
    "    img = np.array(Image.open(img_path)).astype('float32')        \n",
    "    temp.append(img)\n",
    "X_test = np.stack(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ece763f-1e8e-4647-875c-0df134b875d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing image data\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf20ff-1553-4074-af75-a6c448e62fd5",
   "metadata": {},
   "source": [
    "# Label encode the image respective labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d509fc-af0a-411e-8912-5486e5e01e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding 10 output classes\n",
    "encode_X = LabelEncoder()\n",
    "encode_X_fit = encode_X.fit_transform(y_train)\n",
    "y_train = to_categorical(encode_X_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb87ed8-8508-4bb4-8111-434e3a507345",
   "metadata": {},
   "source": [
    "# Define the CNN network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f912eda-f71e-4243-8bd1-50c4f240aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model = keras.models.Sequential([    \n",
    "    # Adding first convolutional-relu layer\n",
    "    # Same padding is used, adding extra rows and columns of pixels to match output feature map with the input\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='relu', \n",
    "                        kernel_regularizer=keras.regularizers.l2(0.001), input_shape=(32, 32, 3), name='Conv_1'),\n",
    "    \n",
    "    # Normalizing the parameters from last layer to speed up the performance (This is just for performance boost so it is optional)\n",
    "    keras.layers.BatchNormalization(name='BN_1'),\n",
    "    \n",
    "    # Adding first pooling layer\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), name='MaxPool_1'),\n",
    "    \n",
    "    # Adding second convolutional-relu layer\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu', \n",
    "                        kernel_regularizer=keras.regularizers.l2(0.001), name='Conv_2'),\n",
    "\n",
    "    # Normalizing the parameters\n",
    "    keras.layers.BatchNormalization(name='BN_2'),\n",
    "    \n",
    "    # Adding second pooling layer\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), name='MaxPool_2'),\n",
    "\n",
    "    # Since CNN is ready then attaching it to go further with traditional neural network\n",
    "    # Flattens the input\n",
    "    keras.layers.Flatten(name='Flat'),\n",
    "    # Fully-Connected layer\n",
    "    keras.layers.Dense(num_classes, activation='softmax', name='pred_layer')    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d4004eb-ef0d-47fb-80cf-18a261386689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv_1 (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " BN_1 (BatchNormalization)   (None, 32, 32, 32)        128       \n",
      "                                                                 \n",
      " MaxPool_1 (MaxPooling2D)    (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " Conv_2 (Conv2D)             (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " BN_2 (BatchNormalization)   (None, 16, 16, 64)        256       \n",
      "                                                                 \n",
      " MaxPool_2 (MaxPooling2D)    (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " Flat (Flatten)              (None, 4096)              0         \n",
      "                                                                 \n",
      " pred_layer (Dense)          (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60746 (237.29 KB)\n",
      "Trainable params: 60554 (236.54 KB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ab9d33-5e8a-43c8-8dea-8557fcee5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "875/875 [==============================] - ETA: 0s - loss: 1.6857 - accuracy: 0.4850WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "875/875 [==============================] - 45s 49ms/step - loss: 1.6857 - accuracy: 0.4850 - val_loss: 1.8924 - val_accuracy: 0.4617\n",
      "Epoch 2/5\n",
      "875/875 [==============================] - ETA: 0s - loss: 1.2065 - accuracy: 0.6122WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "875/875 [==============================] - 43s 50ms/step - loss: 1.2065 - accuracy: 0.6122 - val_loss: 1.4175 - val_accuracy: 0.5537\n",
      "Epoch 3/5\n",
      "875/875 [==============================] - ETA: 0s - loss: 1.0311 - accuracy: 0.6650WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "875/875 [==============================] - 45s 51ms/step - loss: 1.0311 - accuracy: 0.6650 - val_loss: 1.3281 - val_accuracy: 0.5804\n",
      "Epoch 4/5\n",
      "874/875 [============================>.] - ETA: 0s - loss: 0.9247 - accuracy: 0.7038WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "875/875 [==============================] - 46s 52ms/step - loss: 0.9251 - accuracy: 0.7037 - val_loss: 1.3351 - val_accuracy: 0.5974\n",
      "Epoch 5/5\n",
      "875/875 [==============================] - ETA: 0s - loss: 0.8472 - accuracy: 0.7280WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "875/875 [==============================] - 46s 52ms/step - loss: 0.8472 - accuracy: 0.7280 - val_loss: 1.2902 - val_accuracy: 0.6156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2550b119990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.Adam(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cpfile = r'CIFAR10_checkpoint.hdf5' # Weights to be stored in HDF5 format\n",
    "\n",
    "cb_checkpoint = keras.callbacks.ModelCheckpoint(cpfile, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "epochs = 5\n",
    "model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, callbacks=[cb_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761b1c8-2ccf-49c7-8afa-df0efd292068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
